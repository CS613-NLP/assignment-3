{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIo7L9ay8mf_",
        "outputId": "c5f14a41-a72a-4bf3-cc91-30c80854f788"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "! pip install evaluate\n",
        "! pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S24GNsnnfX9E",
        "outputId": "090e9ef8-bdd8-4590-bfba-8fa31cafa4ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at Skratch99/finetuned-bert-squadv2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Skratch99/bert-pretrained\", from_tf = True)\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"Skratch99/finetuned-bert-squadv2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "2xeZXYQsfcxW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric, concatenate_datasets\n",
        "\n",
        "raw_datasets = load_dataset(\"squad_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "lgYwM3UjkmgD"
      },
      "outputs": [],
      "source": [
        "def remove_no_answers(row):\n",
        "    if len(row[\"answers\"][\"text\"]) > 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def remove_no_start(row):\n",
        "    if len(row[\"answers\"][\"answer_start\"]) > 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "raw_datasets = raw_datasets.filter(remove_no_answers)\n",
        "raw_datasets = raw_datasets.filter(remove_no_start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "dcyBgRVUL9fS"
      },
      "outputs": [],
      "source": [
        "t1 = raw_datasets[\"train\"]\n",
        "t2 = raw_datasets[\"validation\"]\n",
        "\n",
        "raw_datasets = concatenate_datasets([t1, t2])\n",
        "\n",
        "split_dataset = raw_datasets.class_encode_column(\"title\").train_test_split(\n",
        "    test_size=0.2, stratify_by_column=\"title\", seed=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "-OJDYMzvRjpX"
      },
      "outputs": [],
      "source": [
        "test_dataset = split_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "8m8WDFEO69MH"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhSEYlSK4dbA",
        "outputId": "e0bd84d1-1360-4a32-8194-37f3da424277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1855/1855 [24:28<00:00,  1.26it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "def process_example(example):\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "\n",
        "\n",
        "    # Check if all expected answers are the same\n",
        "    # for expected_answer in set(expected_answer_list):\n",
        "        # Your question answering model pipeline or function (replace qa_pipeline with your actual function)\n",
        "    predicted_answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "        # Append predictions in the required format\n",
        "    predictions.append({'prediction_text': predicted_answer['answer'], 'id': example['id'], 'no_answer_probability': 0.})\n",
        "\n",
        "    references.append({\"id\": example[\"id\"], \"answers\": example[\"answers\"]})\n",
        "\n",
        "# Number of parallel processes, adjust as needed\n",
        "num_processes = 5\n",
        "\n",
        "# Using concurrent.futures to parallelize the processing\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
        "    list(tqdm(executor.map(process_example, test_dataset), total=len(test_dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "32PAC0nxOT69"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('p9.pkl', 'wb') as f:\n",
        "  pickle.dump(predictions, f)\n",
        "\n",
        "with open('r9.pkl', 'wb') as f:\n",
        "  pickle.dump(references, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LArXrzqIODU6",
        "outputId": "10dc0075-f4f4-44d3-fbb4-2219700d222d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'exact': 10.878706199460916, 'f1': 18.90159908670889, 'total': 18550, 'HasAns_exact': 10.878706199460916, 'HasAns_f1': 18.90159908670889, 'HasAns_total': 18550, 'best_exact': 10.878706199460916, 'best_exact_thresh': 0.0, 'best_f1': 18.90159908670889, 'best_f1_thresh': 0.0}\n",
            "Rouge:  {'rouge1': AggregateScore(low=Score(precision=0.3880581739816478, recall=0.4550323675149823, fmeasure=0.40900635426960424), mid=Score(precision=0.38935330249693045, recall=0.4567322690085111, fmeasure=0.4103497252640948), high=Score(precision=0.3906760301519545, recall=0.45862590291124583, fmeasure=0.4116893438014193)), 'rouge2': AggregateScore(low=Score(precision=0.12528948929063083, recall=0.150201841445336, fmeasure=0.13317084526081496), mid=Score(precision=0.12632324748328383, recall=0.1514989027008819, fmeasure=0.13426731369454203), high=Score(precision=0.12755253063395253, recall=0.15298924635620328, fmeasure=0.13557166179535804)), 'rougeL': AggregateScore(low=Score(precision=0.27524800173164926, recall=0.32233724286555226, fmeasure=0.2899624542456411), mid=Score(precision=0.27616365718365277, recall=0.3235875644282537, fmeasure=0.2908665929400386), high=Score(precision=0.2771266845676933, recall=0.324837436794631, fmeasure=0.29187489300822833)), 'rougeLsum': AggregateScore(low=Score(precision=0.2752319220963203, recall=0.322357382059127, fmeasure=0.289915588313673), mid=Score(precision=0.27619126243883463, recall=0.32358788119911713, fmeasure=0.2908832134862902), high=Score(precision=0.2772057732795161, recall=0.3247918159208708, fmeasure=0.29187799639799783))}\n",
            "F1 Score:  18.90159908670889\n",
            "Exact Match:  10.878706199460916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meteor:  34.42349882500762\n",
            "{'bleu': 0.07474411444487124, 'precisions': [0.16116218190673323, 0.08790346524154896, 0.055196789710064045, 0.03991398129744876], 'brevity_penalty': 1.0, 'length_ratio': 1.0560288019544184, 'translation_length': 65704, 'reference_length': 62218}\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"squad_v2\")\n",
        "vals = metric.compute(predictions=predictions, references=references)\n",
        "print(vals)\n",
        "\n",
        "metric = load_metric(\"rouge\")\n",
        "rouge = metric.compute(predictions=predictions, references=references)\n",
        "print(\"Rouge: \", rouge)\n",
        "\n",
        "print(\"F1 Score: \", vals['f1'])\n",
        "print(\"Exact Match: \", vals['exact'])\n",
        "\n",
        "\n",
        "\n",
        "metric = load_metric(\"meteor\")\n",
        "meteor = metric.compute(predictions=predictions, references=references)\n",
        "print(\"Meteor: \",meteor['meteor']*100)\n",
        "\n",
        "\n",
        "preds = []\n",
        "theo = []\n",
        "for i,j in zip(predictions,references):\n",
        "  preds.append(i['prediction_text'])\n",
        "  theo.append(j['answers']['text'])\n",
        "metric = evaluate.load(\"bleu\")\n",
        "bleu =metric.compute(predictions=preds, references=theo)\n",
        "\n",
        "print(bleu)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
